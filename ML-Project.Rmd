---
title: "Practical ML Project"
author: "C. Klopfenstein"
date: "July 25, 2015"
output: html_document
---

## Summary

We analyze data on Human Activity Reccognition from http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201 (E. Velloso et al.). Data from sensors attached to subjects while lifting weights is classified according to "how well" the activity was performed - with results falling into 5 categories. 159 features are measured for each observation. After preprocessing the data with Principal Components Analysis, the number of features can be reduced to 25. Modeling this data using a Random Forest algorithm results in a prediction accuracy of 96.7%.

<!-- Body -->

```{r, setup, echo = FALSE, eval = TRUE, message = FALSE, error = FALSE, comment = FALSE}

setwd("~/Devel/R/DataScience/ML/Project")

library(dplyr)
library(data.table)
library(ggplot2)
library(gridExtra)
library(AppliedPredictiveModeling)
library(caret)

set.seed(7654321) # for reproducibility

trainFile <- "./data/pml-training.csv"
testFile <- "./data/pml-testing.csv"

dtTrain <- read.table(trainFile, header = TRUE, sep = ",")
dtTest <- read.table(testFile, header = TRUE, sep = ",")
```

## Data Processing

Both the training and testing data are obtained from http://groupware.les.inf.puc-rio.br/har. The training data consists of 19622 observations of 160 variables (159 features, plus the classification), the test data consists of 20 observations of 160 variables. For the analysis, we want to reduce the number of features and eliminate redundancy. We will use Principal Component Analysis (PCA) to accomplish that. But first, we can reduce the number of features by inspection: many features are mostly NA (factor) values, and may not be very useful. The more meaningful features have names beginning with: roll, pitch, yaw, total, gyros, accel, magnet. Selecting those features yields a set of 52 (numeric) features.

We can now apply PCA to these features - for a threshold of 0.80, we find 12 principal components. For a threshold of 0.95, we find 25 principal components. (The threshold is the amount of variance in the data accounted for by the principal components - a higher threshold implies more principal components).

The training data set is further divided into a training set, and a validation set, with 80% of the input going into training, and 20% set aside for cross-validation.

```{r, select, echo = FALSE, eval = TRUE, message = FALSE, error = FALSE, comment = FALSE}
# some problems with data.table, so convert it to data frame, and try
# everything again

dfTrain <- as.data.frame.matrix(dtTrain)
dfTest <- as.data.frame.matrix(dtTest)

inTrain <- createDataPartition(dfTrain$classe, p = 0.8, list = FALSE)
training <- dfTrain[inTrain,]
validation <- dfTrain[-inTrain,]

numericCols <- grep("^(roll|pitch|yaw|total|gyros|accel|magnet).*", 
                    names(dfTrain), value = TRUE)
# select 52 of original 159 columns

preProc <- subset(training, TRUE, numericCols)
trainData <- cbind(training[,"classe"], preProc)
colnames(trainData)[1] = "classe"

preProc <- subset(validation, TRUE, numericCols)
valData <- cbind(validation[,"classe"], preProc)
colnames(valData)[1] = "classe"

```

## Model Building and Cross-Validation

We can now try to model the training data, using the R caret package. Many options are available, for example linear discriminant analysis (lda), quadratic discriminant analysis (qda),  simple classification trees (rpart), bagging - i.e. bootstrap aggregation (treebag), and random forests (rf).

Several models are fit to the training set, then evaluated against the validation set. The following table shows the accuracy obtained for both the training and validation sets.

The accuracy given for the validation set is determined by comparing the predicted values (from fitting the training set) with the actual values in the validation set.

```{r, lda, echo = FALSE, eval = TRUE, message = FALSE, error = FALSE, comment = FALSE, warning = FALSE, cache =TRUE}
# try linear discriminant analysis
ldaPCA95 <- train(classe ~ ., data = trainData, method = "lda",
                  preProcess = "pca",
                  trControl = trainControl(preProcOptions = list(thresh = 0.95)))
# runs in seconds, but only 53% accuracy
#confusionMatrix(ldaPCA95, newdata=trainData)
#ldaPCA95
# cross-validation
predLdaPCA95 <- predict(ldaPCA95, valData)
cmLdaPCA95 <- confusionMatrix(predLdaPCA95, valData$classe)

```

```{r, qda, echo = FALSE, eval = TRUE, message = FALSE, error = FALSE, comment = FALSE, warning = FALSE, cache = TRUE}
# and quadratic discriminant analysis
qdaPCA95 <- train(classe ~ ., data = trainData, method = "qda",
                  preProcess = "pca",
                  trControl = trainControl(preProcOptions = list(thresh = 0.95)))
# runs in seconds, but only 75% accuracy
#confusionMatrix(qdaPCA95, newdata=trainData)
#qdaPCA95

predQdaPCA95 <- predict(qdaPCA95, valData)
cmQdaPCA95 <- confusionMatrix(predQdaPCA95, valData$classe)

```

```{r, rpart, echo = FALSE, eval = TRUE, message = FALSE, error = FALSE, comment = FALSE, warning = FALSE, cache = TRUE}
# classification tree
rpartPCA95 <- train(classe ~ ., data = trainData, method = "rpart",
                    preProcess = "pca",
                    trControl = trainControl(preProcOptions = list(thresh = 0.95)))

#confusionMatrix(rpartPCA95, newdata=trainData)

# cross-validation
predPartPCA95 <- predict(rpartPCA95, valData)
cmRpartPCA95 <- confusionMatrix(predPartPCA95, valData$classe)

```
```{r, bagcompute, echo = FALSE, eval = TRUE, message = FALSE, error = FALSE, comment = FALSE, warning = FALSE, cache = TRUE}
# also try bagging - use method treebag - again, with PCA95
bagPCA95 <- train(classe ~ ., data = trainData, method = "treebag",
                  preProcess = "pca",
                  trControl = trainControl(preProcOptions = list(thresh = 0.95)))
# runs in 5 - 10 minutes
```

```{r, bagdisplay, echo = FALSE, eval = TRUE, message = FALSE, error = FALSE, comment = FALSE, warning = FALSE, cache = FALSE}
#confusionMatrix(bagPCA95, newdata=trainData)
#bagPCA95
# cross-validation
predBagPCA95 <- predict(bagPCA95, valData)
cmBagPCA95 <- confusionMatrix(predBagPCA95, valData$classe)

```

```{r, rfcompute, echo = FALSE, eval = TRUE, message = FALSE, error = FALSE, comment = FALSE, warning = FALSE, cache = TRUE}
# lengthy computation, so cache this chunk

# random forest
# set number of resamples to 10 (think that default is 30)
# set PCA threshold to 0.8 (12 principal components)
rfPCA80 <- train(classe ~ ., data = trainData, method = "rf",
                 preProcess = "pca",
                 trControl = trainControl(preProcOptions = list(thresh = 0.80),
                                          number = 10))
# set PCA threshold to 0.95 (25 principal components)
rfPCA95 <- train(classe ~ ., data = trainData, method = "rf",
                 preProcess = "pca",
                 trControl = trainControl(preProcOptions = list(thresh = 0.95),
                                          number = 10))

```

```{r, rfdisplay, echo = FALSE, eval = TRUE, message = FALSE, error = FALSE, comment = FALSE, warning = FALSE, cache = FALSE}
# threshold = 0.80
#confusionMatrix(rfPCA80, newdata=trainData)
#rfPCA80

# cross-validation
predRfPCA80 <- predict(rfPCA80, valData)
cmRfPCA80 <- confusionMatrix(predict(rfPCA80, valData), valData$classe)

# threshold = 0.95
#confusionMatrix(rfPCA95, newdata=trainData)
#rfPCA95

# cross-validation
predRfPCA95 <- predict(rfPCA95, valData)
cmRfPCA95 <- confusionMatrix(predict(rfPCA95, valData), valData$classe)
```



Model | Accuracy (training) | Accuracy (validation) | Out-of-sample error
---------------|---------------------|--------------------|------------
LDA, PCA(0.95) | `r round(ldaPCA95$results$Accuracy, 3)` | `r round(cmLdaPCA95$overall[1], 3)` | `r (1 - round(cmLdaPCA95$overall[1], 3))`
QDA, PCA(0.95) | `r round(qdaPCA95$results$Accuracy, 3)` | `r round(cmQdaPCA95$overall[1], 3)` | `r (1 - round(cmQdaPCA95$overall[1], 3))`
RPART, PCA(0.95) | `r round(rpartPCA95$results$Accuracy[1], 3)` | `r round(cmRpartPCA95$overall[1], 3)` | `r (1 - round(cmRpartPCA95$overall[1], 3))`
TREEBAG, PCA(0.95) | `r round(bagPCA95$results$Accuracy, 3)` | `r round(cmBagPCA95$overall[1], 3)` | `r (1 - round(cmBagPCA95$overall[1], 3))`
RF, PCA(0.80) | `r round(rfPCA80$results$Accuracy[1], 3)` | `r round(cmRfPCA80$overall[1], 3)` | `r (1 - round(cmRfPCA80$overall[1], 3))`
RF, PCA(0.95) | `r round(rfPCA95$results$Accuracy[1], 3)` | `r round(cmRfPCA95$overall[1], 3)` | `r (1 - round(cmRfPCA95$overall[1], 3))`

We can see that the re-sampling models - TREEBAG, RF - do better than simple classification trees or discriminant analysis. 'Random Forest' outperforms 'Bagging', but the computation takes about 2x longer.

The best model tried is Random Forest, with PCA preprocessing (threshold = 0.95), the model output and confusion matrix for the validation set are shown:

```{r, bestdisplay, echo = FALSE, eval = TRUE, message = FALSE, error = FALSE, comment = FALSE, warning = FALSE, cache = FALSE}
rfPCA95
cmRfPCA95
```

## Results

The Random Forest model with PCA (threshold = 0.95) is used to predict classification values for the test set. 19 / 20 values are predicted correctly (in line with the cross-validation accuracy of 
`r round(cmRfPCA95$overall[1], 3)`).
